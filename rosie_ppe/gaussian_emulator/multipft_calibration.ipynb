{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46afd705-6b62-41ee-b9e0-b77c59251c70",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2023b/lib/python3.10/site-packages/xarray/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing, tutorial\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     load_dataarray,\n\u001b[1;32m      4\u001b[0m     load_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     save_mfdataset,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzarr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_zarr\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import netCDF4 as nc4\n",
    "import shutil\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd526d-012c-4a8c-81c9-e867c78b3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d363e3-1f38-4201-b532-01aaacdee99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bfcfd-1cfa-4685-8bf2-74998d7add89",
   "metadata": {},
   "source": [
    "### 1. Set control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579637b-6c90-4d25-86bb-a3a1f784996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doanalysis = 1\n",
    "paramfileroot='/glade/u/home/rfisher/rosiefork_fates_global_cal/rosie_ppe/nocomp_parameter_modification_python/paramfiles/NOCOMP_LHC/'\n",
    "convgpp = 3600*24*1000 # KgC.m2/s to gc/m2/day\n",
    "convet=3600*24\n",
    "\n",
    "#where are we now?\n",
    "read_params = 1\n",
    "ncls_pft=5\n",
    "\n",
    "USER='rfisher'\n",
    "output_dir='/glade/derecho/scratch/'+USER+'/'\n",
    "yr='.clm2.h0.'   \n",
    "\n",
    "sgpath='sgmap.nc'\n",
    "\n",
    "ppe_root='/glade/u/home/rfisher/rosiefork_fates_global_cal/rosie_ppe'\n",
    "pfts=[0, 1,2,10, 11] #removed the two shrub PFTs \n",
    "#pfts=[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f646f6d-5b1f-44ff-ac40-9da11913682a",
   "metadata": {},
   "source": [
    "### Set ensemble specific control variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add64b36-a1ee-4264-809d-7c7b7cb454c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensN=1\n",
    "\n",
    "if ensN == 1:\n",
    "    spmode=1\n",
    "    ncases=50\n",
    "    ychoose =2000\n",
    "    read_model_output=1\n",
    "    ncls_param=4\n",
    "    read_params = 1\n",
    "    make_params=0\n",
    "    pfileroot=ppe_root+'/nocomp_parameter_modification_python/paramfiles/SP_LHC/sp_lhc_v1_'  \n",
    "    ens_directory='FATES_SP_LHC_ALLPFTS1_e_'   \n",
    "    tf='-02.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971d934-8e0b-48b8-82f1-1f96efca2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(spmode==0):\n",
    "    vars_bm=['ET','GPP','LAI','CUE']\n",
    "    vars_read=['FATES_GPP','FATES_NPP','FATES_LAI','FATES_NOCOMP_PATCHAREA_PF','QFLX_EVAP_TOT','lat','lon']\n",
    "\n",
    "else:\n",
    "    vars_bm=['ET','GPP']\n",
    "    vars_read=['FATES_GPP','FATES_LAI','QFLX_EVAP_TOT','lat','lon']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb04789-a9f1-4c84-b2b2-093a2930e20d",
   "metadata": {},
   "source": [
    "### Derived control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47292d10-ed12-4fd9-8e10-dde4bd4e5a64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdatafile='parameter_outputs/params_v'+str(ensN)+'.nc'\n",
    "\n",
    "vs=range(1,ncases+1) \n",
    "\n",
    "if(ychoose<10):\n",
    "    hstring='.clm2.h0.000'\n",
    "elif (ychoose<100):\n",
    "    hstring='.clm2.h0.00'\n",
    "elif ychoose<1000:\n",
    "    hstring='.clm2.h0.0' \n",
    "else:\n",
    "    hstring='.clm2.h0.'\n",
    "print('hstring',hstring)\n",
    "\n",
    "pft_index=np.zeros(ncases*len(pfts))\n",
    "print('pftindex',np.shape(pft_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc7e51-0cf7-4ebb-b9d5-cc49a53b638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(fileroot):   \n",
    "    if(os.path.isdir(fileroot)):\n",
    "        print('dir exists:'+fileroot)\n",
    "    else:\n",
    "        os.mkdir(fileroot)\n",
    "        print('made: '+fileroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3495f-5e45-4617-a8cb-26ccd385cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "figdir = (os.getcwd() +'/'+ 'figs_'+ens_directory+'/')\n",
    "make_directory(figdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a851078-90ad-4785-aa17-190046103ca8",
   "metadata": {},
   "source": [
    "###  2. Make a single data structure for the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde29d2-02b5-43c9-865e-2922020fb57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b5bb3-997e-43d6-ba2f-4b72f6dece56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_model_output)\n",
    "if read_model_output==1:\n",
    "    print(output_dir,ncases+1)\n",
    "    caseroot=ens_directory\n",
    "    vs=range(ncases)\n",
    "    print('pfts',pfts)\n",
    "    if doanalysis ==1:\n",
    "        debug=1\n",
    "        \n",
    "        dsc_pft=[]\n",
    "        firstgo=1\n",
    "        count=0\n",
    "        missing=np.multiply(range(0,ncases+1),0)\n",
    "        for pft in pfts:\n",
    "            print('pft',pft)\n",
    "            for i in range(1,ncases+1):\n",
    "                print(count,i)\n",
    "                run=caseroot+str(i)+'_PFT_'+str(pft)\n",
    "                print('run',run)\n",
    "                pft_index[count]=pft\n",
    "                count=count+1\n",
    "                ahpath = output_dir + 'archive/' + run + '/lnd/hist/' \n",
    "                rhpath = output_dir + run + '/run/'                 \n",
    "                tfile = run+hstring+str(ychoose)+tf\n",
    "                tfile = run+hstring+str(ychoose)+'-02.nc' \n",
    "                tfileall= run+hstring+str(ychoose)+'-*.nc' \n",
    "                 \n",
    "                missing[i]=1\n",
    "                if(os.path.isfile(rhpath+tfile)):  # this years is in the archive. \n",
    "                    hpath = rhpath\n",
    "                    if debug == 1 :print('file in rundir',rhpath)\n",
    "                    missing[i]=0\n",
    "                else:\n",
    "                    if debug == 1 :print('file not in rundir. try archive')                \n",
    "                    hpath = ahpath       \n",
    "                    if(os.path.isdir(hpath)): \n",
    "                        #print('there is a rundir',hpath+tfile)                \n",
    "                        if(os.path.isfile(hpath+tfile)):\n",
    "                            if debug == 1 :print('file in  archive')\n",
    "                            missing[i]=0\n",
    "                        else:\n",
    "                            print('no file in rundir',hpath+tfile)\n",
    "                    else:\n",
    "                        print('there is no  rundir',hpath+tfile)\n",
    "                    \n",
    "                if(missing[i]==0): # the year can be found. \n",
    "                    rt=hpath+run+yr+str(ychoose)+'*'\n",
    "                    rt=hpath+tfileall\n",
    "                    outf=hpath+'cat_file_'+str(i)+'.nc'\n",
    "                    print('rt',rt)\n",
    "                    !rm $outf\n",
    "                    !ncrcat $rt $outf\n",
    "                    if debug==1: \n",
    "                        \n",
    "                        print(rt)\n",
    "                    if(firstgo==1):\n",
    "                        tmp = xr.open_mfdataset(rt, decode_times=False)  \n",
    "                        allvars=list(xr.open_dataset(hpath+tfile, decode_times=False).variables)\n",
    "                        dropvars=list(set(allvars) - set(vars_read)) \n",
    "                        firstgo=0\n",
    "                    #print('rt',rt)\n",
    "                    tmp=xr.open_mfdataset(outf, decode_times=False, drop_variables=dropvars)\n",
    "                else:\n",
    "                    print('MISSING INPUT')\n",
    "                  \n",
    "                if i==1:\n",
    "                    print('i=1')\n",
    "                    try: \n",
    "                        del dsc\n",
    "                    except:\n",
    "                        print('no dsc')\n",
    "                    dsc = tmp.load()           \n",
    "                else:\n",
    "                    dsc=xr.concat([dsc,tmp],'ens')\n",
    "            dsc_pft.append(dsc)\n",
    "    \n",
    "    print('end')\n",
    "else:\n",
    "    print('not reading model output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aaebb8-caf5-4822-9f95-be704d0358c9",
   "metadata": {},
   "source": [
    "### 3 Read parameter files into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db456ec-f85a-45fc-9982-424100e7fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(read_params==1):\n",
    "    vsp=range(1,ncases+1) \n",
    "    print(pfileroot)\n",
    "    paramsp=['not found'] * 100\n",
    "    if doanalysis ==1:\n",
    "        # loop round whole ensemble\n",
    "        for i in vsp: \n",
    "            pfile1= xr.open_dataset(pfileroot+str(i)+'.nc')  \n",
    "            if i==vsp[0]:\n",
    "                try: \n",
    "                    del dsc_params\n",
    "                except:\n",
    "                    print('no dsc')\n",
    "                all_param_data = pfile1                      \n",
    "            else:\n",
    "                all_param_data=xr.concat([all_param_data, pfile1], \"ens\")  \n",
    "                 # Close the file\n",
    "            pfile1.close()\n",
    "else:\n",
    "    print('readging existing param file: ',pdatafile)\n",
    "    all_param_data= xr.open_dataset(pdatafile) \n",
    "    \n",
    "\n",
    "#### Write parameter output file\n",
    "\n",
    "if(read_params==1):\n",
    "    try:\n",
    "        os.remove(pdatafile)\n",
    "    except:\n",
    "        print('no pfile')\n",
    "        \n",
    "    all_param_data.to_netcdf(pdatafile)  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c7708-00ac-4367-ade8-e6be939a4c09",
   "metadata": {},
   "source": [
    "#### 4. Find modified parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be073731-f9e0-4ae4-b5a6-ab66b92d67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = pfileroot\n",
    "data1= xr.open_dataset(pfileroot+str(1)+'.nc') \n",
    "data2= xr.open_dataset(pfileroot+str(2)+'.nc') \n",
    "common_variables = set(data1.variables) & set(data2.variables)\n",
    "\n",
    "# Iterate through the common variables and compare their values\n",
    "parlist=[]\n",
    "for var_name in common_variables:\n",
    "    var_data1 = data1[var_name]\n",
    "    var_data2 = data2[var_name]\n",
    "    # Compare the values of the variables\n",
    "    if not var_data1.equals(var_data2):\n",
    "        parlist.append(var_name)\n",
    "\n",
    "lhc_vars=parlist\n",
    "\n",
    "if(ensN==8):\n",
    "    lhc_vars=['fates_mort_hf_sm_threshold',\n",
    "     'fates_phen_gddthresh_b', 'fates_mort_scalar_cstarvation', \n",
    "    'fates_grperc', 'fates_leaf_slamax', 'fates_phen_cold_size_threshold',\n",
    "    'fates_phen_drought_threshold','fates_allom_d2ca_coefficient_min',\n",
    "    'fates_rad_leaf_clumping_index',  'fates_leaf_slatop', \n",
    "    'fates_mort_scalar_coldstress','fates_allom_d2bl1',\n",
    "    'fates_allom_d2ca_coefficient_max', 'fates_maintresp_leaf_vert_scaler_coeff2', \n",
    "    'fates_mort_scalar_hydrfailure', 'fates_mort_freezetol', 'fates_phen_mindayson',   \n",
    "    'fates_leafn_vert_scaler_coeff2', 'fates_allom_fnrt_prof_b', \n",
    "    'fates_allom_d2bl2', 'fates_stoich_nitr', 'fates_maintresp_leaf_vert_scaler_coeff1', 'fates_phen_coldtemp',\n",
    "    'fates_turnover_leaf', 'fates_recruit_seed_supplement', 'fates_q10_mr', 'fates_turnover_fnrt']\n",
    "    parlist=lhc_vars\n",
    "print(lhc_vars)\n",
    "print(len(parlist))\n",
    "\n",
    "#if ensN == 10: \n",
    "   # lhc_vars.remove(\"fates_q10_mr\")    \n",
    "   # lhc_vars.remove(\"fates_phen_gddthresh_b\")  \n",
    "    \n",
    "print(len(lhc_vars))\n",
    "lhc_vars_title=np.copy(lhc_vars)\n",
    "for i, var in enumerate(lhc_vars):\n",
    "    \n",
    "    original_string =var\n",
    "    substring_to_remove = \"fates_\"\n",
    "    lhc_vars_title[i] = var.replace(substring_to_remove, \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4828f25c-a298-4357-a34b-ba6c4c5f5baf",
   "metadata": {},
   "source": [
    "### 5. Find parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d3217-56ed-45d5-9918-7105a0d46c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ensn=all_param_data.dims['ens']\n",
    "x_array = np.zeros((ensn,len(parlist)))\n",
    "plhc=all_param_data[parlist]\n",
    "for i, var in enumerate(parlist):\n",
    "    nd=all_param_data[var].ndim\n",
    "    dnames=all_param_data[var].dims\n",
    "\n",
    "    if(nd==1):\n",
    "        param_mean=all_param_data[var] \n",
    "    elif nd==2:\n",
    "        param_mean=all_param_data[var].mean('fates_pft')\n",
    "    elif nd==3:\n",
    "        param_mean=all_param_data[var].mean('fates_pft') \n",
    "        param_mean=param_mean.mean(dnames[1])\n",
    "    x_array[:, i] = param_mean\n",
    "    print('i=',i,var,x_array[1:4,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a901d-deeb-41b8-853f-2db9651d66e7",
   "metadata": {},
   "source": [
    "### Write (and read) output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bc632-4f2b-46dd-9c19-755aa00751ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_in_pft=[]\n",
    "gpp_in_pft=[]\n",
    "lai_in_pft=[]\n",
    "cue_in_pft=[]\n",
    "\n",
    "countp=0\n",
    "for pft in pfts:\n",
    "    ldatafile='outputs/lai_output_'+str(ensN)+'_PFT-'+str(pft)+'.nc'\n",
    "    etdatafile='outputs/et_output_'+str(ensN)+'_PFT-'+str(pft)+'.nc'\n",
    "    cuedatafile='outputs/cue_output_'+str(ensN)+'_PFT-'+str(pft)+'.nc'\n",
    "    gppdatafile='outputs/gpp_output_'+str(ensN)+'_PFT-'+str(pft)+'.nc'\n",
    "\n",
    "    if(read_model_output==1):\n",
    "        try:\n",
    "            os.remove(ldatafile)\n",
    "        except:\n",
    "            print('no lfile')\n",
    "        try:\n",
    "            os.remove(etdatafile)\n",
    "        except:\n",
    "            print('no etfile')\n",
    "        try:\n",
    "            os.remove(gppdatafile)\n",
    "        except:\n",
    "            print('no gppfile')\n",
    "        try:\n",
    "            os.remove(cuedatafile)\n",
    "        except:\n",
    "            print('no cuefile')\n",
    "        dsc=dsc_pft[countp]\n",
    "        countp=countp+1\n",
    "        etmatrix=(dsc['QFLX_EVAP_TOT'].mean('time') )\n",
    "        etmatrix.to_netcdf(etdatafile)\n",
    "        \n",
    "        print(gppdatafile)\n",
    "        gppmatrix=(dsc['FATES_GPP'].mean('time') )\n",
    "        gppmatrix.to_netcdf(gppdatafile) \n",
    "    \n",
    "        print(ldatafile)\n",
    "        laimatrix=(dsc['FATES_LAI'].mean('time') )\n",
    "        laimatrix.to_netcdf(ldatafile) \n",
    "        \n",
    "        if(spmode==0):\n",
    "            print(cuedatafile)\n",
    "            cuematrix=np.divide(dsc['FATES_NPP'].mean('time'),dsc['FATES_GPP'].mean('time') )\n",
    "            cuematrix.to_netcdf(cuedatafile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce6a63-0886-44e9-ae38-3ab597340062",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pfts)\n",
    "\n",
    "for pft in pfts:\n",
    "    ldatafile='outputs/lai_output_'+str(ensN)+'_PFT-'+str(pft)+'.nc'\n",
    "    etdatafile='outputs/et_output_'+str(ensN)+'_PFT-'+str(pft)+'.nc'\n",
    "    cuedatafile='outputs/cue_output_'+str(ensN)+'_PFT-'+str(pft)+'.nc'\n",
    "    gppdatafile='outputs/gpp_output_'+str(ensN)+'_PFT-'+str(pft)+'.nc'\n",
    "    print(ldatafile)\n",
    "    et_in=xr.open_dataset(etdatafile)\n",
    "    et_in=np.multiply(et_in,convet)    \n",
    "    gpp_in=xr.open_dataset(gppdatafile)\n",
    "    gpp_in=np.multiply(gpp_in,convgpp)\n",
    "    lai_in=xr.open_dataset(ldatafile)\n",
    "    print('lai_in',pft,np.shape(lai_in.FATES_LAI))\n",
    "    et_in_pft.append(et_in)\n",
    "    gpp_in_pft.append(gpp_in)\n",
    "    lai_in_pft.append(lai_in)\n",
    "    et_in.close()\n",
    "    gpp_in.close()\n",
    "    lai_in.close()\n",
    "    print('var in',pft,np.shape(gpp_in.FATES_GPP))\n",
    "\n",
    "    if(spmode==0):\n",
    "        cuein=xr.open_dataset(cuedatafile)\n",
    "        cue_in_pft.append(cue_in)\n",
    "        cuein.close()\n",
    "\n",
    "for i in range(len(pfts)):\n",
    "    l1=lai_in_pft[i].FATES_LAI\n",
    "    print(i,l1.dims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd693da-ce58-42a2-b682-a5304c885072",
   "metadata": {},
   "source": [
    "### Find the values for the meshfile gridpoints in the ILAMB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ef0f2-15a7-4c0c-8730-9a8f97327d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "gppfile_m='/glade/work/rfisher/ILAMB_data/DATA/gpp/FLUXCOM/gpp.nc'\n",
    "gppfile_w='/glade/work/rfisher/ILAMB_data/DATA/gpp/WECANN/gpp.nc'\n",
    "laifile='/glade/work/rfisher/FATES_calibration/ILAMB_data/DATA/lai/MODIS/lai_0.5x0.5.nc'\n",
    "etfile='/glade/work/rfisher/FATES_calibration/ILAMB_data/DATA/evspsbl/GLEAMv3.3a/et.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a068b-3986-4d43-81d3-05163650bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open ilamb data. \n",
    "ds_gpp_m = xr.open_dataset(gppfile_m)\n",
    "ds_gpp_w = xr.open_dataset(gppfile_w)\n",
    "ds_lai = xr.open_dataset(laifile)\n",
    "ds_et = xr.open_dataset(etfile)\n",
    "ds_gpp_m=ds_gpp_m.mean('time')\n",
    "ds_gpp_w=ds_gpp_w.mean('time')\n",
    "ds_lai=ds_lai.mean('time')\n",
    "ds_lai['lat']=ds_lai.lat*-1\n",
    "ds_et=ds_et.mean('time')\n",
    "ds_et=ds_et*convet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8994a35-91b8-41d1-97fd-ab3ba657c3c1",
   "metadata": {},
   "source": [
    "### Find and open mesh file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eefd39-4e36-4bf1-8891-13b83f962223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "meshroot='/glade/work/rfisher/FATES_calibration/mesh_files/'\n",
    "meshfiles= os.listdir(meshroot)\n",
    "lat_list_pft=[]\n",
    "lon_list_pft=[]\n",
    "\n",
    "for pft in pfts:\n",
    "    target_string = 'pft'+str(pft)+'_'\n",
    "    meshfile = [file for file in meshfiles if target_string in file]\n",
    "    print(pft,meshfile)\n",
    "    mesh=xr.open_dataset(meshroot+meshfile[0])\n",
    "    latlon=mesh.where(mesh.elementMask == 1, drop=True).centerCoords.values\n",
    "    #Convert the longitudes in the mesh file from 0-360 space to -180 to 180 space \n",
    "    #to be comparable to the ILAMB data files\n",
    "    lat_mesh=latlon[:,1]\n",
    "    lon_mesh=latlon[:,0]\n",
    "    lat_list=lat_mesh\n",
    "    lon_list=lon_mesh\n",
    "    npoints=len(lon_list)\n",
    "    for i in range(npoints):\n",
    "        if(lon_list[i]>180):        \n",
    "            lon_list[i]=(360-lon_list[i])*-1\n",
    "    lon_list_pft.append(lon_list)\n",
    "    lat_list_pft.append(lat_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee5fca-0fba-4497-8ca0-eb44b140eb88",
   "metadata": {},
   "source": [
    "### Extract the GPP and ET and LAI values for the lat lons in the mesh file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb90ba-ae48-4bc0-909a-08f1ca1a97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for pft in pfts:\n",
    "    plt.hist(lat_list_pft[counter])\n",
    "    counter=counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca9e16-f22c-4aea-9391-d3e1bb1ab509",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for pft in pfts:\n",
    "    plt.hist(lat_list_pft[counter])\n",
    "    counter=counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513209f-6a54-4ec2-bace-a9635c296639",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xarr_lat_gpp = ds_gpp_m['lat']\n",
    "Xarr_lon_gpp = ds_gpp_m['lon']\n",
    "Xarr_lat_et = ds_et['lat']\n",
    "Xarr_lon_et = ds_et['lon']\n",
    "ds_lai['lat']=-ds_lai['lat']\n",
    "Xarr_lat_lai = ds_lai['lat']\n",
    "Xarr_lon_lai = ds_lai['lon']\n",
    "\n",
    "npft=len(pfts)\n",
    "array2d = np.zeros((3, 4))\n",
    "\n",
    "gpp_wecann_SG_pft=[]\n",
    "gpp_SG_pft=[]\n",
    "et_SG_pft =[]\n",
    "lai_SG_pft=[]\n",
    "\n",
    "countp=0\n",
    "for pft in pfts:\n",
    "    nlat=int(len(lon_list_pft[countp]))\n",
    "    gpp_SG=np.zeros(nlat)\n",
    "    lai_SG=np.zeros(nlat)\n",
    "    et_SG=np.zeros(nlat)\n",
    "    print(nlat,np.shape(lai_SG),np.shape(lon_list_pft[countp]))\n",
    "\n",
    "    for i in range(len(lat_list_pft[countp])):\n",
    "        lat_i=lat_list_pft[countp][i]\n",
    "        lon_i=lon_list_pft[countp][i]\n",
    "        nearest_index_lat = np.abs(Xarr_lat_gpp - lat_i).argmin()\n",
    "        nearest_index_lon = np.abs(Xarr_lon_gpp - lon_i).argmin()\n",
    "        gpp_SG[i]=ds_gpp_m['gpp'][nearest_index_lat,nearest_index_lon]\n",
    "              \n",
    "        nearest_index_lat_et = np.abs(Xarr_lat_et - lat_i).argmin()\n",
    "        nearest_index_lon_et = np.abs(Xarr_lon_et - lon_i).argmin()\n",
    "        et_SG[i]=ds_et['et'][nearest_index_lat_et,nearest_index_lon_et]\n",
    "     \n",
    "        nearest_index_lat_lai = np.abs(Xarr_lat_lai - lat_i).argmin()\n",
    "        nearest_index_lon_lai = np.abs(Xarr_lon_lai - lon_i).argmin()\n",
    "        lai_SG[i]=ds_lai['lai'][nearest_index_lat_lai,nearest_index_lon_et]\n",
    "\n",
    "    et_SG_pft.append(et_SG)\n",
    "    gpp_SG_pft.append(gpp_SG)         \n",
    "    lai_SG_pft.append(lai_SG) \n",
    "    print(pft,countp,np.shape(lai_SG_pft[countp]))\n",
    "    fig = plt.figure(figsize=(25, 15))         \n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.coastlines()\n",
    "    ds_lai.lai.plot(ax=ax, transform=ccrs.PlateCarree(),  vmin=0, cmap='viridis',add_colorbar=False)\n",
    "    scatter = ax.scatter(lon_list_pft[countp], lat_list_pft[countp], c='white', cmap='viridis', marker='o', s=6, edgecolor='black',linewidth=0.5,transform=ccrs.PlateCarree())\n",
    "    print('maxlai',np.max(ds_lai.lai))\n",
    "    cbar = plt.colorbar(scatter, shrink=0.5)\n",
    "    #plt.show()\n",
    "    countp=countp+1\n",
    "    figname=figdir+'/PFT_points_'+str(pft)+'.png'\n",
    "    plt.savefig(figname)  \n",
    "    \n",
    "for i in range(len(pfts)):\n",
    "    print(i,np.shape(lai_SG_pft[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3dd46-9edf-48fb-9957-944dc98bae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#surface dataset\n",
    "fsurdat=xr.open_dataset('/glade/campaign/cesm/cesmdata/inputdata/lnd/clm2/surfdata_esmf/ctsm5.2.0/surfdata_1.9x2.5_hist_2000_16pfts_c240216.nc')\n",
    "lai_ann=fsurdat.MONTHLY_LAI.mean('time')\n",
    "pft=2\n",
    "#lai_ann.isel(lsmpft=pft).plot()\n",
    "#fsurdat.MONTHLY_LAI.mean('time')[3].plot()\n",
    "pftl=[1,2]\n",
    "for pft in [1, 2]:\n",
    "    target_string = 'pft'+str(pft)+'_'\n",
    "    meshfile = [file for file in meshfiles if target_string in file]\n",
    "    print(pft,meshfile)\n",
    "    mesh=xr.open_dataset(meshroot+meshfile[0])\n",
    "    latlon=mesh.where(mesh.elementMask == 1, drop=True).centerCoords.values\n",
    "    #Convert the longitudes in the mesh file from 0-360 space to -180 to 180 space \n",
    "    #to be comparable to the ILAMB data files\n",
    "    lat_mesh=latlon[:,1]\n",
    "    lon_mesh=latlon[:,0]\n",
    "\n",
    "    plt.scatter(lon_mesh, lat_mesh, c='white', cmap='viridis', marker='o', s=6, edgecolor='black',linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cedb6-edfb-48f1-8a75-c6ece0a9dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsurdat.lsmlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954b936-c4d1-4a25-ba88-6c57a80cc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsurdat.PCT_NAT_PFT[1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159dbb3f-0489-4c51-8a02-963bba48d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lai_ent1=np.multiply(fsurdat.MONTHLY_LAI.mean('time')[1],fsurdat.PCT_NAT_PFT[1])/100\n",
    "lai_ent2=np.multiply(fsurdat.MONTHLY_LAI.mean('time')[2],fsurdat.PCT_NAT_PFT[2])/100\n",
    "\n",
    "(lai_ent1+lai_ent1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815434b4-ab7f-4cba-8e13-483602997c55",
   "metadata": {},
   "source": [
    "### 10.1 Plot the LAI ensemble space per PFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fabf6-0954-4d42-ba93-d6d9f328f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pfts)):\n",
    "    print(i,np.shape(lai_SG_pft[i]))\n",
    "    print(i,np.max(lai_in_pft[i].FATES_LAI))\n",
    "    print(i, np.shape(lai_SG_pft[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d40a00-b945-4948-b311-244959364b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ensemble_range(var_in_pft,var,var_SG_pft):\n",
    "    plot_range=1\n",
    "    ncls= ncls_pft\n",
    "    rw=max(1,int(len(pfts)/ncls))\n",
    "\n",
    "    if(plot_range==1): \n",
    "        fig, axes = plt.subplots(nrows=rw, ncols=ncls, figsize=(25, 8))\n",
    "        plt.subplots_adjust(wspace=0.15, hspace=0.25)\n",
    "    \n",
    "        axcount=0\n",
    "        countp=0\n",
    "        for p in pfts: # pfts to select. \n",
    "            col = countp % ncls\n",
    "            axc=col   \n",
    "            if(rw>1):\n",
    "                row = axcount //ncls\n",
    "                axc=[row,col]\n",
    "            varens=var_in_pft[countp][var]\n",
    "            ngcells=len(np.mean(varens,0))\n",
    "            xgcells=range(ngcells)\n",
    "            xvar = [list(xgcells) for _ in range(len(varens))]\n",
    "    \n",
    "            if np.shape(varens)[1] <20:\n",
    "                for e in range(ncases): \n",
    "                    axes[axc].scatter(xgcells,varens,color=plt.cm.tab10(e),s=18,label='ens #'+str(e))\n",
    "            else:\n",
    "                axes[axc].scatter(xvar,varens,color='grey',s=8)\n",
    "            axes[axc].scatter(xgcells, var_SG_pft[countp],color='blue',s=100,marker='x')        \n",
    "            axes[axc].set_ylabel(var, fontsize=26) \n",
    "           # axes[axc].set_ylim([0,9]) \n",
    "            axes[axc].set_title('PFT'+str(p), fontsize=26) \n",
    "            axes[axc].set_xlabel('Gridcell', fontsize=20)\n",
    "            countp=countp+1\n",
    "\n",
    "        #while axcount < rw*ncls:\n",
    "        #    row = axcount //ncls\n",
    "        #    col = axcount %ncls\n",
    "        #    axes[axc].remove()\n",
    "            axcount += 1\n",
    "        figname=figdir+'/range_vs_data_'+var+'_y'+str(ychoose)+'Nens_'+str(ensN)+'.png'\n",
    "        plt.savefig(figname,bbox_inches='tight')  \n",
    "        plt.show()\n",
    "        print(figname)\n",
    "        countp=countp+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea3b65-3f03-499a-8938-4061f1d6d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('sh1',np.shape(gpp_in_pft[1]))\n",
    "plot_ensemble_range(gpp_in_pft,'FATES_GPP',gpp_SG_pft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0074a8-18a5-47b1-8ea0-63716c825b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ensemble_range(lai_in_pft,'FATES_LAI',lai_SG_pft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629fd84-c7ed-4374-ae03-7a6012c49039",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ensemble_range(et_in_pft,'QFLX_EVAP_TOT',et_SG_pft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe110c3b-f845-42b0-88f2-176c0b42f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc_pft[0].FATES_LAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a4ca3d-bf74-4390-9f9c-592702a0f157",
   "metadata": {},
   "source": [
    "### Surrogate Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d444aef-a170-4015-b010-03fade6ee96f",
   "metadata": {},
   "source": [
    "### 13.2 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd371f8-143f-485d-b2cd-7365926ca580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_regression(g,fgs, axes, row, col,vnonan):\n",
    "    yall=vnonan[:,g]\n",
    "    if np.ndim(yall)==2:   \n",
    "        yall=np.mean(yall,1)\n",
    "    nan_indices = yall.isnull()\n",
    "    yall[np.isnan(yall)] = np.mean(yall)   \n",
    "    X=x_array\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,yall, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = sm.OLS(y_train, X_train).fit()  # Fit the multivariate linear regression model\n",
    "   # print(model.summary())\n",
    "    predictions = model.predict(X_test)\n",
    "    tr_predictions = model.predict(X)\n",
    "    if(fgs==1):\n",
    "        #map_param_space(g)\n",
    "        axes[row,col].scatter(y_test,predictions,color='green',label='Linear Regression')\n",
    "        #plt.scatter(tr_predictions,y)\n",
    "    residuals = model.resid\n",
    "\n",
    "# Calculate the standard deviation of predictions\n",
    "  #  std_dev_predictions = residuals.std()\n",
    "  #  rel_dev=np.divide(std_dev_predictions,np.mean(y))\n",
    "\n",
    "    r_squared = stats.pearsonr(predictions, y_test)[0] ** 2\n",
    "    return r_squared\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b4190-14da-4271-83f0-76bd67430d48",
   "metadata": {},
   "source": [
    "### 13.3 Gaussian Process emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b85c8d-ff47-4c25-a3dc-ea3751c64a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_emulator_for_point(g,fgs, axes, row, col,vn):\n",
    "    yall=vn[:,g]\n",
    "\n",
    "    if np.ndim(yall)==2:     \n",
    "        yall=np.mean(yall,1)\n",
    "    nan_indices = yall.isnull()\n",
    "    yall[np.isnan(yall)] = np.mean(yall)\n",
    "    yall=yall[0:len(x_array)]\n",
    "    X=x_array[0:len(yall),:]\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,yall, test_size=0.2, random_state=42)\n",
    "    mask = np.where(np.isinf(y_train))\n",
    "    y_train[mask]=0.5    \n",
    "    kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "    y_pred =gp.fit(X_train, y_train)\n",
    "    test_prediction, std_prediction = gp.predict(X_test, return_std=True)\n",
    "    #r2 = r2_score(ytest, test_prediction) \n",
    "    \n",
    "    if(np.max(test_prediction)==0):\n",
    "        print('emulator fail')\n",
    "    else:\n",
    "        print('mean pred',np.mean(test_prediction))\n",
    "    \n",
    "\n",
    "    if(fgs==1 ):\n",
    "        if(multp==1):\n",
    "            ax=axes[row,col]\n",
    "        else:\n",
    "            ax=axes\n",
    "            \n",
    "        ax.scatter(y_test,test_prediction, color='red', marker='o',label='GP emulator')\n",
    "        ax.legend(loc='upper left')\n",
    "        #ax.scatter(test_prediction,ytest)\n",
    "        #ax.set_xlabel('LAI emulator')\n",
    "        #ax.set_ylabel('LAI FATES')\n",
    "\n",
    "    r_squared = stats.pearsonr(test_prediction, y_test)[0] ** 2\n",
    "    return r_squared\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a44c9e-475a-481c-9ed1-4e792e4f1a57",
   "metadata": {},
   "source": [
    "###  Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c600f9b-fcd8-4026-a7b0-09c3ea845abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a sample dataset (Boston housing dataset)\n",
    "def random_forest(fgs,feat, axes,axc,multp,vnonan):\n",
    "    yall=vnonan\n",
    "\n",
    "    if np.ndim(yall)==2:     \n",
    "        yall=np.mean(yall,1)\n",
    "    nan_indices = yall.isnull()\n",
    "    yall[np.isnan(yall)] = np.mean(yall)\n",
    "    yall=yall[0:len(x_array)]\n",
    "    X=x_array[0:len(yall),:]\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,yall, test_size=0.2, random_state=42)\n",
    "    mask = np.where(np.isinf(y_train))\n",
    "    y_train[mask]=0.5\n",
    "    # Create a random forest regressor model\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_models.append(rf_model)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    print('ypred',np.min(y_pred),np.max(y_pred))\n",
    "    if(fgs==1 ):\n",
    "        if(multp==1):\n",
    "            ax=axes[axc]\n",
    "        else:\n",
    "            ax=axes[axc]\n",
    "       # map_param_space(g)\n",
    "        #fig = plt.figure()\n",
    "        print('axc',axc)\n",
    "\n",
    "        axes[axc].scatter(y_test,y_pred,color='blue',s=20,label='Random Forest')\n",
    "        axes[axc].plot([min(y_pred),max(y_pred)],[min(y_pred),max(y_pred)],color='black')\n",
    "            #plt.scatter(tr_predictions,y)\n",
    "        #axes[row,col].title('g='+str(g)+' Lat:'+str(round(glat[g], 2))+' Lon:'+str(round(glon[g], 2)))\n",
    "\n",
    "    r_squared = stats.pearsonr(y_pred, y_test)[0] ** 2\n",
    "    if(feat==1):\n",
    "        feature_importances = rf_model.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame to display the feature importances\n",
    "        #feature_importances_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "        #feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "        feature_importances_df = pd.DataFrame({'Feature Importance': feature_importances})\n",
    "        feature_importances_df['FeatureIndex'] = range(0, len(feature_importances_df) )\n",
    "\n",
    "        feature_importances_df = feature_importances_df.sort_values(by='Feature Importance', ascending=False)\n",
    "    \n",
    "        # Display the dominant factors\n",
    "        #print(feature_importances_df.FeatureIndex.iloc[1:4])\n",
    "        featimp=feature_importances_df.FeatureIndex.iloc[0:4]\n",
    "    return rf_model,r_squared,featimp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb874a7-5c97-47ef-ad48-a377693a284e",
   "metadata": {},
   "source": [
    "    \n",
    "### 15. Make a large hypercube encompassing the whole parameter space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edebee-0101-4976-a2e9-6266e2df737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=5000\n",
    "num_variables = len(lhc_vars)\n",
    "hypercube = np.zeros((num_samples, num_variables))\n",
    "for i in range(num_variables):\n",
    "    hypercube[:, i] = np.random.permutation(np.linspace(0, 1, num_samples))\n",
    "\n",
    "hypercube_scaled=np.zeros((num_samples, num_variables))\n",
    "#for i  in range(5):\n",
    "for i, var in enumerate(lhc_vars):\n",
    "    mx=np.max(x_array[:,i])\n",
    "    mn=np.min(x_array[:,i])\n",
    "    print(var,mn,mx)\n",
    "    for e in range(num_samples):\n",
    "        hypercube_scaled[e,i]=(hypercube[e,i]*(mx-mn))+mn\n",
    "Xdense=hypercube_scaled\n",
    "print('hypercube shape',np.shape(Xdense))\n",
    "### 16 Find simulations with acceptable LAI and CUE\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab189c-58b1-48ac-bae3-40565a87f9e8",
   "metadata": {},
   "source": [
    "### 14.  Run emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b72fcc-6813-4d25-9473-4185a1219f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make empty lists for statistical models\n",
    "rf_st=np.zeros(355)\n",
    "lr_st=np.zeros(355)\n",
    "gp_st=np.zeros(355)\n",
    "rf_models_et=[]\n",
    "rf_models_gpp=[]\n",
    "rf_models_lai=[]\n",
    "rf_models_cue=[]\n",
    "\n",
    "y_pred_noise_et_pft=[]\n",
    "y_pred_noise_gpp_pft=[]\n",
    "y_pred_noise_lai_pft=[]\n",
    "y_pred_noise_cue_pft=[]\n",
    "\n",
    "ncls=ncls_pft\n",
    "rw=max(int(len(pfts)/ncls)+1,1)\n",
    "rw=1\n",
    "print('ncls,rw',ncls,rw)\n",
    "feat_importance = np.zeros([len(pfts),4,3], dtype=int)\n",
    "\n",
    "vnonan=[et_in_pft,gpp_in_pft]\n",
    "vars=['QFLX_EVAP_TOT','FATES_GPP']\n",
    "if(spmode==0):\n",
    "    vars=['QFLX_EVAP_TOT','FATES_GPP','FATES_LAI','FATES_NPP']\n",
    "    vnonan=[et_in_pft,gpp_in_pft,lai_in_pft,cue_in_pft]   \n",
    "    \n",
    "for v, vn in enumerate(vnonan): \n",
    "    fig, axes = plt.subplots(nrows=rw, ncols=ncls, figsize=(14, 3))\n",
    "    plt.subplots_adjust(wspace=0.25, hspace=0.12)\n",
    "    multp=1\n",
    "    rf_models = []\n",
    "    rf_st=np.zeros(max(pfts))\n",
    "    axcount=0\n",
    "    for i in range(len(pfts)):\n",
    "        var_pft=vn[axcount][vars[v]]           \n",
    "        p=pfts[i]\n",
    "        print('p,v',p,v)\n",
    "        col = axcount % ncls \n",
    "        axc=col   \n",
    "        if(rw>1):\n",
    "            row = axcount //ncls\n",
    "            axc=[row,col]  \n",
    "        axcount=axcount+1\n",
    "        print('axc',axc)\n",
    "        rf_model,rsq,feat_importance[i,:,v]=random_forest(1,1,axes, axc,multp,var_pft)\n",
    "        #make_emulator_for_point(gcells_pft[p],1, axes, row, col,vn)\n",
    "        print('rsq',rsq,i,axc)\n",
    "        axes[axc].set_xlabel(vars_bm[v]+' process model',fontsize=12)\n",
    "        axes[axc].set_ylabel(vars_bm[v]+' surrogate',fontsize=12)\n",
    "      # axes[axc].set_title(sg_data['pft'].values[p].title(),fontsize=16)\n",
    "        if(v==0):        \n",
    "            y_pred_noise_et_pft.append(rf_model.predict(Xdense))\n",
    "        elif v==1:\n",
    "            y_pred_noise_gpp_pft.append(rf_model.predict(Xdense))\n",
    "        if(spmode==0): \n",
    "            if v==2:\n",
    "                y_pred_noise_lai_pft.append(rf_model.predict(Xdense))\n",
    "            elif v==3:\n",
    "                y_pred_noise_cue_pft.append(rf_model.predict(Xdense))\n",
    "    print(axcount)                \n",
    "    while axcount < rw*ncls:\n",
    "        col = axcount % ncls \n",
    "        axc=col   \n",
    "        if(rw>1):\n",
    "            row = axcount //ncls\n",
    "            axc=[row,col]  \n",
    "        axes[axc].remove()\n",
    "        axcount += 1\n",
    "\n",
    "    plt.savefig(figdir+'/PFT_level_RF_fit_'+vars_bm[v]+'_y'+str(ychoose)+'Nens_'+str(ensN)+'.png',bbox_inches='tight')  \n",
    "    plt.show()\n",
    "\n",
    "print(feat_importance[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246d0a8-0d30-4091-9591-d141c45b77a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bad4fc5d-7156-4569-9d31-59ea9e23e140",
   "metadata": {},
   "source": [
    "### Find mean observed LAI, GPP, ET per PFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee27d81-02bf-4503-b9dc-23b69fecbb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "et_pft=np.zeros(max(pfts)+1)\n",
    "lai_pft=np.zeros(max(pfts)+1) \n",
    "gbaf_gpp_pft=np.zeros(max(pfts)+1)\n",
    "\n",
    "countp=0\n",
    "for p in pfts:\n",
    "    mask = np.isfinite(et_SG_pft[countp])\n",
    "    et_pft[p] = np.mean(et_SG_pft[countp][mask])\n",
    "    mask = np.isfinite(gpp_SG_pft[countp])\n",
    "    gbaf_gpp_pft[p] = np.mean(gpp_SG_pft[countp][mask])\n",
    "    if(spmode==0):\n",
    "        mask = np.isfinite(lai_SG_pft[countp])\n",
    "        lai_pft[p] = np.mean(lai_SG_pft[countp][mask])\n",
    "    countp=countp+1\n",
    "print(et_pft)\n",
    "print(gbaf_gpp_pft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78531b-42ea-4664-876b-b2c9f4be6b7a",
   "metadata": {},
   "source": [
    "### Find acceptable models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80661ced-bc62-45d2-971a-ea64f9be622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def find_acceptable_models(rfm,p,pftfit,vartoggle,fates_ens,fates_or_surrogate):\n",
    "    fates_ens_mean=fates_ens.mean('gridcell')\n",
    "    fates_ens_mean.plot() \n",
    "    print('fatesens',np.shape(fates_ens),np.shape(fates_ens_mean),fates_ens.dims)\n",
    "    if vartoggle==0:\n",
    "        y_pred_noise=y_pred_noise_et_pft[rfm]\n",
    "        etmin=et_pft[p]-0.1*et_pft[p]\n",
    "        etmax=et_pft[p]+0.1*et_pft[p]\n",
    "        print('et minmax',np.min(y_pred_noise),np.max(y_pred_noise))\n",
    "        y_accept=np.where(np.logical_and(y_pred_noise>=etmin, y_pred_noise<=etmax))   \n",
    "        y_accept_ens= np.where(np.logical_and(fates_ens_mean>=etmin, fates_ens_mean<=etmax))  \n",
    "        print('et range',etmin,etmax,et_pft[p],np.shape(y_accept))\n",
    "    elif vartoggle==1:\n",
    "        y_pred_noise=y_pred_noise_gpp_pft[rfm]\n",
    "        gppmin=gbaf_gpp_pft[p]-0.1*gbaf_gpp_pft[p]\n",
    "        gppmax=gbaf_gpp_pft[p]+0.1*gbaf_gpp_pft[p]\n",
    "        y_accept=np.where(np.logical_and(y_pred_noise>=gppmin, y_pred_noise<=gppmax))   \n",
    "        y_accept_ens= np.where(np.logical_and(fates_ens_mean>=gppmin, fates_ens_mean<=gppmax))  \n",
    "        print('gpprange',gppmin,gppmax,np.shape(y_accept))\n",
    "    if spmode==0:\n",
    "        if(vartoggle==2): \n",
    "            y_pred_noise=y_pred_noise_lai_pft[rfm]\n",
    "            if(pftfit==1):\n",
    "                ldmin=lai_pft[p]-0.1*lai_pft[p]\n",
    "                ldmax=lai_pft[p]+0.1*lai_pft[p]\n",
    "            y_accept=np.where(np.logical_and(y_pred_noise>=ldmin, y_pred_noise<=ldmax))   \n",
    "            y_accept_ens= np.where(np.logical_and(fates_ens_mean>=ldmin, fates_ens_mean<=ldmax))  \n",
    "            print('lai range',p,ldmin,ldmax,np.shape(y_accept))\n",
    "            print('lai range emul',p,np.min(y_pred_noise),np.max(y_pred_noise),np.shape(y_accept))\n",
    "        elif vartoggle==3:\n",
    "            y_pred_noise=y_pred_noise_cue_pft[rfm]\n",
    "            cuemin=cue_pft_mean[p]-0.5*cue_pft_std[p]\n",
    "            cuemax=cue_pft_mean[p]+0.5*cue_pft_std[p]  \n",
    "            y_accept=np.where(np.logical_and(y_pred_noise>=cuemin, y_pred_noise<=cuemax))   \n",
    "            y_accept_ens= np.where(np.logical_and(fates_ens_mean>=cuemin, fates_ens_mean<=cuemax))  \n",
    "            print('cuerange',cuemin,cuemax,np.shape(y_accept))\n",
    "\n",
    "    if(fates_or_surrogate==0):\n",
    "        return y_accept_ens\n",
    "    else:\n",
    "        return y_accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05568974-6666-433f-be10-da1dcf52baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pftfit=1\n",
    "\n",
    "accept_lai_i = []\n",
    "accept_et_i = []\n",
    "accept_gpp_i = []\n",
    "accept_cue_i = []\n",
    "accept_both_i = []\n",
    "\n",
    "accept_lai_ens_i = []\n",
    "accept_et_ens_i = []\n",
    "accept_gpp_ens_i = []\n",
    "accept_cue_ens_i = []\n",
    "accept_both_ens_i = []\n",
    "\n",
    "\n",
    "if(pftfit==1):\n",
    "    for rfm in range(len(pfts)):        \n",
    "        p=pfts[rfm]   \n",
    "        print('pft',p)\n",
    "\n",
    "        accept_et_i.append(find_acceptable_models(rfm,p,pftfit,0,et_in_pft[rfm][vars[0]],1))\n",
    "        accept_gpp_i.append(find_acceptable_models(rfm,p,pftfit,1,gpp_in_pft[rfm][vars[1]],1))\n",
    "        accept_et_ens_i.append(find_acceptable_models(rfm,p,pftfit,0,et_in_pft[rfm][vars[0]],0))\n",
    "        accept_gpp_ens_i.append(find_acceptable_models(rfm,p,pftfit,1,gpp_in_pft[rfm][vars[1]],0))\n",
    "\n",
    "        if(spmode==0):              \n",
    "            accept_lai_i.append(find_acceptable_models(rfm,p,pftfit,2,lai_in_pft[rfm][vars[2]],1))\n",
    "            accept_cue_i.append(find_acceptable_models(rfm,p,pftfit,3,cue_in_pft[rfm][vars[3]],1))\n",
    " \n",
    "            accept_lai_ens_i.append(find_acceptable_models(rfm,p,pftfit,2,lai_in_pft[rfm][vars[2]],0))\n",
    "            accept_cue_ens_i.append(find_acceptable_models(rfm,p,pftfit,3,cue_in_pft[rfm][vars[3]],0))\n",
    "        \n",
    "else: # gridcell fit  \n",
    "    for rfm in range(len(gcells)):\n",
    "        g=gcells[i]        \n",
    "        accept_lai_i.append(find_acceptable_models(rfm,g,pftfit))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f3d80-eb6a-4f1e-8f1d-16bfcc58ff76",
   "metadata": {},
   "source": [
    "### Draw selected parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ead5e-73a8-4e06-9f78-26ab7e3cfcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_selected_param_space(rfm,p,pftfit,vtoggle):\n",
    "    ncls=ncls_param\n",
    "    if ncls==5: \n",
    "        tsz=16\n",
    "        ysz=14\n",
    "        xsz=14\n",
    "        ht=16\n",
    "        psz=9\n",
    "    if ncls==4 :\n",
    "        tsz=18\n",
    "        ysz=18\n",
    "        xsz=18\n",
    "        ht=13\n",
    "        psz=14\n",
    "    #colors = plt.get_cmap('tab10').colors\n",
    "    #colors = plt.get_cmap('CSS4_COLORS').colors\n",
    "\n",
    "    # Choosing a specific color from the Tableau color palette\n",
    "          \n",
    "    axcount=0\n",
    "\n",
    "    y_accept_et=accept_et_i[rfm]\n",
    "    y_accept_gpp=accept_gpp_i[rfm]\n",
    "    if(spmode==0):\n",
    "        y_accept_cue=accept_cue_i[rfm]\n",
    "        y_accept_lai=accept_lai_i[rfm]    \n",
    "        \n",
    "    if(vtoggle==0):\n",
    "        y_pred_noise  = y_pred_noise_et_pft[rfm]\n",
    "        fatesens=et_in_pft[rfm].QFLX_EVAP_TOT[0:len(x_array)]\n",
    "        vlab='ET'\n",
    "        minv=et_pft[p]-0.1*et_pft[p] \n",
    "        rangev=0.2*et_pft[p] \n",
    "\n",
    "    elif vtoggle==1:\n",
    "        y_pred_noise  = y_pred_noise_gpp_pft[rfm]\n",
    "        fatesens=gpp_in_pft[rfm].FATES_GPP[0:len(x_array)]\n",
    "        vlab='GPP KgC/m2/year'\n",
    "        minv=gbaf_gpp_pft[p]-0.1*gbaf_gpp_pft[p] \n",
    "        rangev=0.2*gbaf_gpp_pft[p] \n",
    "\n",
    "    print('miv range',minv,range)\n",
    "    if(spmode==0):\n",
    "        if vtoggle==2:\n",
    "            y_pred_noise  = y_pred_noise_lai_pft[rfm]\n",
    "            fatesens=lai_in_pft[rfm].FATES_LAI[0:len(x_array)]\n",
    "            vlab='LAI'\n",
    "        elif vtoggle==3:\n",
    "            y_pred_noise  = y_pred_noise_cue_pft[rfm]\n",
    "            fatesens=lai_in_pft[rfm].FATES_NPP[0:len(x_array)]\n",
    "            vlab='GPP KgC/m2/year'\n",
    "\n",
    "\n",
    "    Xsh=Xdense.shape\n",
    "    rw=int(len(lhc_vars)/ncls)+1\n",
    "    fig, axes = plt.subplots(nrows=rw, ncols=ncls, figsize=(22, ht)) \n",
    "    #pftname=sg_data['pft'].values[p].title()\n",
    "    #fig.suptitle(pftname,fontsize=tsz,y=0.9)\n",
    "    plt.subplots_adjust(wspace=0.15, hspace=0.35)  \n",
    "    pftfit=1\n",
    "    axcount=0\n",
    "    \n",
    "    \n",
    "    for v, var in enumerate(lhc_vars):  \n",
    "        row = axcount //ncls\n",
    "        col = axcount % ncls  \n",
    "        axcount=axcount+1\n",
    "        ap=1\n",
    "\n",
    "        #add a box of the observed range\n",
    "        rangexavars=np.max(Xdense[:,v])-np.min(Xdense[:,v])\n",
    "        rect = patches.Rectangle((np.min(Xdense[:,v]),minv ), rangexavars, rangev, linewidth=1, edgecolor='grey',alpha=0.6, facecolor=chosen_color_target)\n",
    "        axes[row, col].add_patch(rect)\n",
    "        \n",
    "        axes[row, col].scatter(Xdense[:,v], y_pred_noise, color=chosen_color_all,s=2)    \n",
    "\n",
    "        axes[row, col].scatter(Xdense[y_accept_et,v], y_pred_noise[y_accept_et], color=chosen_color_et,s=4, alpha=ap) \n",
    "        axes[row, col].scatter(Xdense[y_accept_gpp,v], y_pred_noise[y_accept_gpp], color=chosen_color_gpp,s=4, alpha=ap)\n",
    "\n",
    "        \n",
    "        if(spmode==0):\n",
    "            axes[row, col].scatter(Xdense[y_accept_lai,v], y_pred_noise[y_accept_lai], color=chosen_color_lai,s=4, alpha=ap) \n",
    "            axes[row, col].scatter(Xdense[y_accept_cue,v], y_pred_noise[y_accept_cue], color=chosen_color_cue,s=4, alpha=ap) \n",
    "\n",
    "        #  Plot out the FATES sensemble outputs. \n",
    "        axes[row, col].scatter(x_array[:,v],  fatesens[:,:].mean(dim='gridcell'), s=psz, edgecolor='black',color='black')\n",
    " \n",
    "        axes[row, col].set_xlabel(lhc_vars_title[v], fontsize=xsz)   \n",
    "        if(col==0):\n",
    "            axes[row, col].set_ylabel(vars_bm[vtoggle], fontsize=ysz)   \n",
    "            \n",
    "    while axcount < rw*ncls:\n",
    "        row = axcount //ncls\n",
    "        col = axcount %ncls\n",
    "        axes[row, col].remove()\n",
    "        axcount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b434a-0360-4a2c-83b3-c9e494579d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import CSS4_COLORS\n",
    "import matplotlib.patches as patches\n",
    "colors = plt.get_cmap('tab10').colors\n",
    "# Choosing a specific color from the CSS4_COLORS dictionary\n",
    "chosen_color_all = CSS4_COLORS['grey']  # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "chosen_color_target = CSS4_COLORS['lightgrey']  # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "\n",
    "chosen_color_et = CSS4_COLORS['gold']  # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "chosen_color_lai = CSS4_COLORS['forestgreen']  # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "chosen_color_ens = CSS4_COLORS['darkblue']  # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "chosen_color_gpp = CSS4_COLORS['cornflowerblue']  # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "chosen_color_cue = CSS4_COLORS['indianred']  # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "\n",
    "#chosen_color_all = colors[0]  # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "chosen_color_cue = colors[1]   # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "chosen_color_lai = colors[4]   # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "chosen_color_ens = colors[5]  # Selecting the 'lightblue' color from the CSS4_COLORS dictionary\n",
    "\n",
    "#one figure for each PFT and variable combination (loops round parameters)\n",
    "if pftfit==1:\n",
    "    print('pfts',pfts)\n",
    "    for i in range(len(pfts)):\n",
    "        for vtoggle, var in enumerate(vars):\n",
    "            p = pfts[i]\n",
    "            #gcells=gcells_pft[p]\n",
    "            print('ip',i,p)\n",
    "            draw_selected_param_space(i,p,pftfit,vtoggle)   \n",
    "    \n",
    "            figname=figdir+'/emulator_and_ensemble_'+str(ensN)+'_'+vars_bm[vtoggle]+'_p'+str(p)+'.png'\n",
    "            print(figname)\n",
    "            plt.savefig(figname, bbox_inches='tight')\n",
    "            plt.show() \n",
    "else:\n",
    "    for i in range(len(gcells_pft[p])):\n",
    "        #g=gcells[i] \n",
    "        print(g,i)\n",
    "        draw_selected_param_space(g,i,pftfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004eacd-4c47-4e35-bad4-27d0fed5d1ea",
   "metadata": {},
   "source": [
    "### Make function to plot 3D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80588d0-0134-449c-b31d-c8c9d5f8edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_param_space(i,g,axes, axc,v,ch):\n",
    "    accept_et=accept_et_i[i]\n",
    "    accept_gpp=accept_gpp_i[i] \n",
    "    if(spmode==0):\n",
    "        accept_lai=accept_lai_i[i] \n",
    "        accept_cue=accept_cue_i[i] \n",
    "        \n",
    "    common_values0 = (accept_et)\n",
    "    common_values1 = np.intersect1d(accept_et, accept_gpp)\n",
    "    if(spmode==0):    \n",
    "        common_values2 = np.intersect1d(common_values1, accept_lai)\n",
    "        common_values3 = np.intersect1d(common_values2, accept_cue)\n",
    "    print('accept et,gpp',np.shape(accept_et),np.shape(accept_gpp))\n",
    "    print('common 0,1',np.shape(common_values0),np.shape(common_values1))\n",
    "\n",
    "    if(v==2):\n",
    "        vf=0\n",
    "    else:\n",
    "        vf=v\n",
    "    vf=1\n",
    "    print(np.shape(accept_gpp),np.shape(accept_et))\n",
    "    xv=feat_importance[i,0,vf]\n",
    "    yv=feat_importance[i,1,vf]\n",
    "    zv=feat_importance[i,2,vf]\n",
    "    cv=feat_importance[i,3,vf]\n",
    "    print(xv,yv,zv,cv)\n",
    "    xse = Xdense[accept_et,xv]\n",
    "    yse = Xdense[accept_et,yv]\n",
    "    zse = Xdense[accept_et,zv]\n",
    "    cse = Xdense[accept_et,cv]\n",
    "    \n",
    "    xsg = Xdense[accept_gpp,xv]\n",
    "    ysg = Xdense[accept_gpp,yv]\n",
    "    zsg = Xdense[accept_gpp,zv]\n",
    "    csg = Xdense[accept_gpp,cv]\n",
    "\n",
    "    xsc0 = Xdense[common_values0,xv]\n",
    "    ysc0 = Xdense[common_values0,yv]\n",
    "    zsc0 = Xdense[common_values0,zv]\n",
    "    csc0 = Xdense[common_values0,cv]\n",
    "\n",
    "    xsc1 = Xdense[common_values1,xv]\n",
    "    ysc1 = Xdense[common_values1,yv]\n",
    "    zsc1 = Xdense[common_values1,zv]\n",
    "    csc1 = Xdense[common_values1,cv]\n",
    "    \n",
    "    if(spmode==0):\n",
    "        xsc2 = Xdense[common_values,xv]\n",
    "        ysc2 = Xdense[common_values,yv]\n",
    "        zsc2 = Xdense[common_values,zv]\n",
    "        csc2 = Xdense[common_values,cv]\n",
    "    \n",
    "        xsc3 = Xdense[common_values,xv]\n",
    "        ysc3 = Xdense[common_values,yv]\n",
    "        zsc3 = Xdense[common_values,zv]\n",
    "        csc3 = Xdense[common_values,cv]    \n",
    "\n",
    "    \n",
    "    if(ch==1):\n",
    "        xsch = Xdense[chosen_values_i[i],xv]\n",
    "        ysch = Xdense[chosen_values_i[i],yv]\n",
    "        zsch = Xdense[chosen_values_i[i],zv]\n",
    "        csch = Xdense[chosen_values_i[i],cv]\n",
    "        \n",
    "    xsd = Xdense[:,xv]\n",
    "    ysd = Xdense[:,yv]\n",
    "    zsd = Xdense[:,zv]\n",
    "    al = 1\n",
    "    sz=12\n",
    "    if(v==0): # ET\n",
    "        plt3d=axes[axc].scatter(xsc0,  ysc0,  zsc0,s=sz, c=chosen_color_et, alpha=al,label='GPP & ET constraint')\n",
    "    elif v==1: #GPP\n",
    "        #plt3d=axes[axc].scatter(xsc0,  ysc0,  zsc0,s=sz+2, c=chosen_color_et, alpha=al,label=' ET constraint')\n",
    "        plt3d=axes[axc].scatter(xsg,  ysg,  zsg,  s=sz,c=chosen_color_gpp, alpha=al,label='+GPP constraint')\n",
    "        #plt3dl=axes[row,col].scatter(xsl,  ysl,  zsl,  s=8,c=chosen_color_lai, alpha=0.7,label='LAI constraint')\n",
    "    if(spmode==0):\n",
    "        if v==2:\n",
    "            #plt3d=axes[axc].scatter(xsc2,  ysc2,  zsc2,  s=8,c='black', alpha=al,label='+LAI constraint')\n",
    "            plt3dl=axes[row,col].scatter(xsl,  ysl,  zsl,  s=8,c=chosen_color_lai, alpha=0.7,label='LAI constraint')\n",
    "        elif v==3:\n",
    "            plt3d=axes[axc].scatter(xsc3,  ysc3,  zsc3,  s=8,c='black', alpha=al,label='+CUE constraint')\n",
    "        #plt3dl=axes[row,col].scatter(xsl,  ysl,  zsl,  s=8,c=chosen_color_lai, alpha=0.7,label='LAI constraint')\n",
    "\n",
    "    if(ch==1):# plot chosen values\n",
    "            plt3d=axes[axc].scatter(xsch,  ysch,  zsch,  s=18,c='red', alpha=al,label='chosen points')\n",
    "        \n",
    "        \n",
    "    fsa=14\n",
    "    fst=16\n",
    "    axes[axc].set_xlabel(lhc_vars_title[xv],fontsize=fsa,labelpad=12)\n",
    "    axes[axc].set_ylabel(lhc_vars_title[yv],fontsize=fsa,labelpad=12)\n",
    "    axes[axc].set_zlabel(lhc_vars_title[zv],fontsize=fsa)\n",
    "    #axes[row,col].set_title(pftname,fontsize=fst)   \n",
    "\n",
    "    return [plt3d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e8b2de-dc99-4044-add0-abe6487b4485",
   "metadata": {},
   "source": [
    "### Plot the 3D space for each PFT average calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0d146-f2e2-4c61-92b3-d1b1f74c68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ncls=ncls_pft\n",
    "rw=int(len(pfts)/ncls)\n",
    "print(ncls,rw)\n",
    "for vtoggle, var in enumerate(vars):\n",
    "    fig, axes = plt.subplots(nrows=rw, ncols=ncls, figsize=(32,27),subplot_kw=dict(projection='3d'))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "\n",
    "    if(pftfit==1):\n",
    "        axcount=0\n",
    "        for i in range(len(pfts)):      \n",
    "            p=pfts[i] \n",
    "            print(p) \n",
    "            col = axcount % ncls \n",
    "            axc=col   \n",
    "            if(rw>1):\n",
    "                row = axcount //ncls\n",
    "                axc=[row,col]  \n",
    "  \n",
    "            hnd=plot_3d_param_space(i,p, axes, axc,vtoggle,0)\n",
    "            axcount=axcount+1\n",
    "    leg_axis=[rw-1,ncls-1]\n",
    "    if(rw>1):\n",
    "        leg_axis = [  ncls-1]  \n",
    "    print(leg_axis)\n",
    "    #ax2D = fig.add_subplot(leg_axis)\n",
    "\n",
    "    #legend=axes[leg_axis].legend(handles=hnd,fontsize=30)  # Place the legend in the upper right corner            \n",
    "    #axes[ rw-1,ncls-1].axis('off')\n",
    "    #for text in legend.get_texts():\n",
    "    #    text.set_color(chosen_color)\n",
    " #   while axcount < rw*ncls:\n",
    " #       row = axcount //ncls\n",
    " #       col = axcount %ncls\n",
    " #       axes[row, col].remove()\n",
    " #       axcount += 1  \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figdir+'/PFT_3D_plot_e'+str(ensN)+'_'+var+'.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2180642-b35d-4328-9e10-438f386d303b",
   "metadata": {},
   "source": [
    "### Choose some members of the selected parameter space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf2c00-9458-41b1-94e8-084da018ff64",
   "metadata": {},
   "source": [
    "#### Create an array of the common values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1de35c-99ba-4b05-bce0-d2869100bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "noutput_files=8\n",
    "\n",
    "chosen_values_pft =[]\n",
    "for i in range(len(pfts)):\n",
    "    p=pfts[i]\n",
    "    accept_gpp=accept_gpp_i[i]\n",
    "    accept_et=accept_et_i[i]   \n",
    "    common_values_p = np.intersect1d(accept_gpp, accept_et)\n",
    "    if(np.size(common_values_p) >= 10):\n",
    "        random_values = random.sample(sorted(common_values_p),noutput_files)  \n",
    "    else:\n",
    "        random_values=[]\n",
    "\n",
    "    print('pft,i',p,i)\n",
    "    print(random_values)\n",
    "    chosen_values_pft.append(random_values)\n",
    "    print(np.shape(chosen_values_pft[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8722fe8-b8c3-4961-ae57-262f1d127322",
   "metadata": {},
   "source": [
    "### For each ensemble member, loop round parameters and change their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73221da5-c5ce-4ded-a034-df93441b17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_params)\n",
    "make_params=0\n",
    "if make_params==1:\n",
    "    print(lhc_vars)\n",
    "    pfile1= pfileroot+str(1)+'.nc'\n",
    "    if(spmode==1):\n",
    "        pout='parameter_outputs/SP_LHC_calibration_'\n",
    "    else:\n",
    "        pout='parameter_outputs/SP_NOCOMP_calibration_'        \n",
    "    pfile1='../nocomp_parameter_modification_python/intermediate_pfiles/modified_average_file2.nc'\n",
    "    \n",
    "    for f in range(noutput_files):\n",
    "        newfile =pout+str(ensN)+'_'+str(f)+'.nc'\n",
    "        print('f',f)\n",
    "        pf=xr.open_dataset(pfile1)\n",
    "        for v, var in enumerate(lhc_vars):\n",
    "            print('var',var)\n",
    "            print((chosen_values_pft[i][:]))\n",
    "            dms=pf[var].dims\n",
    "            ndms=pf[var].ndim\n",
    "            for i in range(len(pfts)):\n",
    "                pft = pfts[i]\n",
    "                print('pft var',pfts[i], var,len(chosen_values_pft[i]))\n",
    "                if len(chosen_values_pft[i])>0 :\n",
    "                    print('yes')\n",
    "\n",
    "                    #print('i,pft',i,f)\n",
    "                    nxd=chosen_values_pft[i][f] # for each PFT (i) and file (f)\n",
    "                    varnew=Xdense[nxd,v] # Find the p1 parameter corresponding                \n",
    "                    if ndms == 0: # one dimensional variable\n",
    "                        print('0d var', pft,var,varnew)    \n",
    "                        #pf[var][pft]=varnew\n",
    "                    elif ndms==1: \n",
    "                        print(pft,varnew)\n",
    "                        pf[var][pft]=varnew\n",
    "                    \n",
    "                    elif ndms == 2: \n",
    "                        vr=pf[var]\n",
    "                        p#rint(vr.values)\n",
    "                        pf[var][0,pft]=varnew \n",
    "                else: #no fitted values, do manual tuning \n",
    "                    print('pft var',pft, var,len(chosen_values_pft[i]))\n",
    "                    print('no')\n",
    "                    if(pft==10 and var == 'fates_leaf_vcmax25top'):\n",
    "                        pf[var][0,pft]=80                      \n",
    "                    if(pft==10 and var == 'fates_leaf_stomatal_intercept'):\n",
    "                        pf[var][pft]=4  \n",
    "                    if(pft==11 and var == 'fates_leaf_vcmax25top'):\n",
    "                        pf[var][0,pft]=20\n",
    "                    if(pft==11 and var == 'fates_leaf_stomatal_intercept'):\n",
    "                        pf[var][pft]=10 \n",
    "                    if(pft==11 and var == 'fates_leaf_stomatal_slope_ballberry'):\n",
    "                        pf[var][pft]=4    \n",
    " \n",
    "                        \n",
    "                    \n",
    "        pf.to_netcdf(newfile)\n",
    "        pf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9db42b-e34b-4569-bc1d-95d9d63ec7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for f in range(noutput_files):\n",
    "f=2\n",
    "\n",
    "       \n",
    "newfile =pout+str(ensN)+'_'+str(f)+'.nc'\n",
    "pf=xr.open_dataset(newfile)\n",
    "\n",
    "for v, var in enumerate(lhc_vars):\n",
    "    print(var,pf[var].values)\n",
    "pf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae7e46-cbf0-41b8-873c-95c799ce3d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-npl-2023b]",
   "language": "python",
   "name": "conda-env-conda-npl-2023b-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
